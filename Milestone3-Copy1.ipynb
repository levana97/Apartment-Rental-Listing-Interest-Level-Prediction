{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Milestone 3\n",
    "### 1. What classifiers did you use for milestone 3? (5 points)\n",
    "\n",
    "1. Gradient Boosting Regressor - Jing Wen\n",
    "2. Random Forest - Zhixuan(Otto) Hu\n",
    "3. Extra Tree - XiaoLu He\n",
    "\n",
    "### 2. Which features did you select for your classifiers? Please comment on the reason for your feature selection. If you choose to work on the bonus question, you can add your features extracted from external datasets at this step. (5 points)\n",
    "\n",
    "GradientBoostingRegressor: Choose bathrooms, bedrooms, price to predict interest_level, number of bathrooms and bedrooms and price has the strongest relationship with interest_level.\n",
    "\n",
    "Extra Tree: We selected 'bathrooms', 'bedrooms', 'price', 'latitude', 'longitude' for extra tree classifier since these features have the strongest relationship with interest_level for extra tree classifier.\n",
    "\n",
    "Random Forest: We choose features: 'price', 'dist_to_city_center','price_per_bedroom', 'price_per_bathroom', 'num_of_features','len_of_description', 'pos_count', 'num_of_photos', 'bedrooms', 'bathrooms'. these feature have relatively high entropy and random forest can process large amount of features so we can use as many as possible.\n",
    "\n",
    "\n",
    "### 3. How did you perform cross-validation? Please describe the procedure. (10 points)\n",
    "\n",
    "Use StratifiedKFold allow me to pass a cv parameter to implement k-fold cross validation. In this case, I choose 5-fold cross validation\n",
    "\n",
    "### 4. What performance did the first version of your classifiers achieve on the validation dataset (in cross-validation) and on the test dataset? Please comment on the performance of the classifier. (15 points: 5 points for performance, and 10 points for comments). \n",
    "\n",
    "Extra tree: ~1.21 on Kaggle due to the overfitting. We found the train loss was only ~0.20, but val loss was ~1.20 and it turned out a high score on Kaggle. The score shows an obvious overfitting. \n",
    "\n",
    "Random forest: 0.74 on kaggle. We set a max depth of 2 and random state of 2. This turns out to be not very effective. Shallow tree don't do a good job. The classifier has poor accuracy on training data and on kaggle.\n",
    "\n",
    "GradientBoostingRegressor: ~1.67 on kaggle, compared with train dataset score 0.713 overfit occurs. Mostly due to not handle inf and large Inverse of regularization strength value(C).\n",
    "\n",
    "### 5. What actions did you take in order to improve your classifiers? You can modify your dataset or the parameters of your classifier. Please record yourmodifications in your report. (30 points: 10 points for each improvement)\n",
    "\n",
    "Extra Tree: We changed 'max_depth' from 'NONE' to '7' to avoid overfitting. When 'max_depth'='NONE', loss on train data set is ~0.20 and loss on validation data set is ~1.20.\n",
    "\n",
    "GradientBoostingRegressor: adjusted the 'n_estimators' value to find the optimal one.\n",
    "\n",
    "Random forest: We removed the depth limitation and added more features. One of Random forest's advantages is that it can have deeper trees without overfitting. Also random forest is able to handle high dimension data.\n",
    "\n",
    "### 6. How did you check whether any overfitting occurred during your training?Did you observe overfitting? What did you do to avoid overfitting? (10 points)\n",
    "\n",
    "Compare the performance on trainning data set and validation dataset, if the performance of the model on trainning data set is much better on the validate dataset, it means overfitting occurs.\n",
    "\n",
    "GradientBoostingRegressor: change the n_estimators parameter of sklearn GradientBoostingRegressor to 1000 from 10000. n_estimators is the number of boosting stages to perform. Gradient boosting is fairly robust to over-fitting so a large number but not as large as 10000 in this case.\n",
    "\n",
    "Extra Tree: We observed overfitting on our first version of our classifier achieve which have ~0.20 loss on train data and ~1.20 loss on validation data. We adjusted the 'max_depth' value from 'NONE' to '7' to avoid overfitting. \n",
    "\n",
    "Random Forest: We adjusted the tree depth and random state. Random forest actually does very well at avoiding overfitting since it uses voting among random trees. I observed very minor overfitting at very large tree depth. Adjusting the random state can reduce overfitting.\n",
    "\n",
    "### 7. What performance did you achieve on the validation dataset (in cross-validation) and on the test dataset after your modifications? Please, try to explain the gains. (15 points: 5 points for performance, and 10 points for explanation)\n",
    "\n",
    "Random forest: 0.65. We removed depth limitation and used a very high random state. ultimately achieved around 0.65.\n",
    "\n",
    "GradientBoostingRegressor: ~0.725. Handle inf value, replace it with mean of the column it belongs to.\n",
    "\n",
    "Extra tree: ~0.73 on the validation set and ~0.73 on the test dataset after I changed 'max_depth' value to '7'.\n",
    "\n",
    "### 8. Evaluate one additional evaluation metrics mentioned in class on the validation dataset. Which metric did you use? What were the results? How do these results compare to the results for multi-class logarithmic loss?(10 points)\n",
    "\n",
    "We evaulated the f score and accuracy score of the classifiers. We found that the lower the log loss the higher the f score and the higher the accuracy score. Our implementations yield around 0.72 for accuracy score and between 0.4 and 0.6 for f score. This is reasonably better than the result we acquired in Milestone 2\n",
    "\n",
    "### 9. Compare your new classifier with the classifiers used in milestone 2. Try to explain the difference between the performance of the classifiers and the gains of the milestone 3 classifiers. (10 points)\n",
    "\n",
    "GradientBoostingRegressorï¼š Compared with Logistic Regression, GradientBoostingRegressor builds an additive model by using forward stage-wise way, it allows for the optimization of arbitrary differentiable loss functions. Unlike Logistic Regression, in each stage a regression tree is fit on the negative gradient of the given loss function.\n",
    "\n",
    "Random Forest: Random forest is built with a \"forest\" of decision trees. The promise is that it will do better against overfitting. Our results show that indeed random forest overfits much less. In our decision tree implementation, at max tree depth of above 5, the overfitting was very severe. With random forest, even with very high (unlimited) tree depth it's able to achieve high accuracy rate. This is because that the random forest classifier generates a bunch of decision trees using randomly selected data and features instead of just generate one tree with all features and all data points. It runs a sample through all trees and collect outcomes from these trees. This avoids overfitting very effectively.\n",
    "\n",
    "Extra tree: Extra tree builds an ensemble of unpruned decision or regression trees according to the classical top-down procedure. It splits nodes by choosing cut-points fully at random and it uses the whole learning sample to grow the trees. It has low variance. As we can see, the scores extremely close on train data and validation data. Compare with svm, it is more accurate and efficient.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "def split_data(X, y):\n",
    "    return train_test_split(X,y)\n",
    "\n",
    "def run_model(clf, dtrain,dtest=None):\n",
    "    if dtest:\n",
    "        clf.fit(dtrain[0],dtrain[1])\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(dtrain[0],dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return clf, y_train_loss\n",
    "\n",
    "# passing split_data according to features\n",
    "def train_cv(clf, X, y, preprocess = 'linear'):\n",
    "    X_train, X_test, y_train, y_test = split_data(X, y)\n",
    "    # print(X_train, X_test)\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    elif preprocess == 'linear':\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "    elif preprocess == 'no_preprocess': \n",
    "        #use original data\n",
    "        pass\n",
    "        \n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "   \n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train.iloc[train_ind, :], X_train.iloc[val_ind, :]\n",
    "        train_y, val_y = y_train.iloc[train_ind], y_train.iloc[val_ind]\n",
    "        \n",
    "        clf, train_loss, val_loss = run_model(clf, (train_x, train_y), (val_x, val_y))\n",
    "\n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss), end=\"\\t\")\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        y_pred = clf.predict(val_x)\n",
    "        accuracy_score = metrics.accuracy_score(val_y, y_pred)\n",
    "        f_score = metrics.f1_score(val_y, y_pred, average='macro')\n",
    "        cv_scores.append([train_loss, val_loss, f_score, accuracy_score])\n",
    "        \n",
    "        print(\"accuracy score: \", accuracy_score)\n",
    "        print(\"f score: \", f_score)\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    \n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))\n",
    "\n",
    "    return clf, cv_scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_features = ['bathrooms', 'bedrooms', 'price']\n",
    "X = data[gb_features]\n",
    "y = data['interest_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13576.8031          23.0494            8.00m\n",
      "         2       13466.9668          21.4350            7.83m\n",
      "         3       13473.6977          22.0284            7.83m\n",
      "         4       13365.9043          19.4552            7.95m\n",
      "         5       13374.5755          18.5239            7.93m\n",
      "         6       13416.9872          17.6293            7.91m\n",
      "         7       13324.9248          16.8784            7.88m\n",
      "         8       13221.9640          15.8464            7.87m\n",
      "         9       13261.0183          14.8054            7.84m\n",
      "        10       13230.4742          14.6534            7.83m\n",
      "        20       12867.4856           8.7760            7.83m\n",
      "        30       12741.3353           5.5346            7.75m\n",
      "        40       12604.9106           3.5317            7.71m\n",
      "        50       12553.6655           2.5082            7.68m\n",
      "        60       12506.7989           1.2291            7.67m\n",
      "        70       12455.6267           1.1913            7.68m\n",
      "        80       12281.6445           0.4424            7.64m\n",
      "        90       12333.9430          -0.0152            7.63m\n",
      "       100       12250.3382           0.2827            7.59m\n",
      "       200       12073.7864          -0.2203            7.38m\n",
      "       300       11978.4090          -0.2308            7.26m\n",
      "       400       11947.7846          -0.3547            7.16m\n",
      "       500       11865.6426          -0.3303            7.08m\n",
      "       600       11746.1584          -0.5361            6.99m\n",
      "       700       11712.1030          -0.5425            6.91m\n",
      "       800       11645.4226          -0.6554            6.84m\n",
      "       900       11649.3083          -0.5924            6.75m\n",
      "      1000       11534.8258          -0.4438            6.68m\n",
      "      2000       11378.6189          -0.3318            5.95m\n",
      "      3000       11128.1508          -0.4714            5.22m\n",
      "      4000       11122.1514          -0.6416            4.48m\n",
      "      5000       11106.2879          -0.7540            3.74m\n",
      "      6000       10964.0108          -0.6082            2.99m\n",
      "      7000       10871.8094          -0.8551            2.25m\n",
      "      8000       10870.7983          -0.6998            1.50m\n",
      "      9000       10843.5590          -0.5999           44.92s\n",
      "     10000       10895.9645          -0.8065            0.00s\n",
      "train_loss: 0.628602, val_loss: 0.787606\ttime elapsed: 0:07:45\n",
      "accuracy score:  0.6794538361508452\n",
      "f score:  0.37314603616017267\n",
      "Running Fold 2 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13604.4274          22.6705            7.83m\n",
      "         2       13515.1276          21.3705            7.83m\n",
      "         3       13376.2470          21.6449            7.72m\n",
      "         4       13424.0895          19.0660            7.75m\n",
      "         5       13402.5630          18.5081            7.76m\n",
      "         6       13378.9216          17.1912            7.72m\n",
      "         7       13294.2323          16.0020            7.68m\n",
      "         8       13236.7584          15.2834            7.74m\n",
      "         9       13215.0960          15.1080            7.77m\n",
      "        10       13180.0061          14.3780            7.76m\n",
      "        20       12930.5299           8.8814            7.74m\n",
      "        30       12710.1524           5.8597            7.69m\n",
      "        40       12529.9806           3.9527            7.67m\n",
      "        50       12443.9802           2.3455            7.66m\n",
      "        60       12421.8423           1.9006            7.63m\n",
      "        70       12436.0845           1.0710            7.59m\n",
      "        80       12310.9228           0.9041            7.57m\n",
      "        90       12383.4669           0.3155            7.58m\n",
      "       100       12192.2575           0.1259            7.55m\n",
      "       200       12092.9511          -0.4280            7.39m\n",
      "       300       12040.4061          -0.3735            7.28m\n",
      "       400       11966.6391          -0.4709            7.19m\n",
      "       500       11907.8199          -0.3213            7.10m\n",
      "       600       11843.8925          -0.6564            7.01m\n",
      "       700       11834.8753          -0.4670            6.93m\n",
      "       800       11718.7527          -0.3069            6.85m\n",
      "       900       11680.8136          -0.5540            6.77m\n",
      "      1000       11681.6087          -0.6573            6.70m\n",
      "      2000       11411.0321          -0.6664            5.96m\n",
      "      3000       11154.8156          -0.6042            5.21m\n",
      "      4000       11140.1513          -0.7239            4.48m\n",
      "      5000       10990.9996          -0.8446            3.73m\n",
      "      6000       10899.7125          -0.7742            2.99m\n",
      "      7000       10930.2451          -0.8739            2.24m\n",
      "      8000       10852.3626          -0.7513            1.50m\n",
      "      9000       10932.8724          -0.9207           44.91s\n",
      "     10000       10818.0126          -0.5000            0.00s\n",
      "train_loss: 0.627229, val_loss: 0.786556\ttime elapsed: 0:07:46\n",
      "accuracy score:  0.6840052015604682\n",
      "f score:  0.38593561689761\n",
      "Running Fold 3 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13626.6446          22.0418            7.83m\n",
      "         2       13475.3899          21.0878            7.83m\n",
      "         3       13504.3046          20.3683            7.83m\n",
      "         4       13468.5206          18.5235            7.87m\n",
      "         5       13470.5069          18.4335            7.80m\n",
      "         6       13402.1094          17.0193            7.80m\n",
      "         7       13290.6368          15.5708            7.73m\n",
      "         8       13313.5224          15.3039            7.74m\n",
      "         9       13288.4719          14.5908            7.72m\n",
      "        10       13184.3329          14.0058            7.69m\n",
      "        20       12943.5224           8.3602            7.63m\n",
      "        30       12804.1735           4.9800            7.62m\n",
      "        40       12610.0275           3.3780            7.60m\n",
      "        50       12406.4775           2.4355            7.57m\n",
      "        60       12462.8311           1.7906            7.54m\n",
      "        70       12416.0936           0.5861            7.53m\n",
      "        80       12313.4467           0.2727            7.51m\n",
      "        90       12351.6897           0.2883            7.50m\n",
      "       100       12321.3250           0.1585            7.48m\n",
      "       200       12163.6210          -0.1282            7.34m\n",
      "       300       12019.4270          -0.2182            7.26m\n",
      "       400       11952.2461          -0.3660            7.15m\n",
      "       500       11853.1991          -0.3540            7.06m\n",
      "       600       11913.0940          -0.5649            6.98m\n",
      "       700       11826.8209          -0.4125            6.90m\n",
      "       800       11742.5772          -0.5047            6.82m\n",
      "       900       11709.2985          -0.4429            6.75m\n",
      "      1000       11675.9667          -0.4176            6.68m\n",
      "      2000       11362.2402          -0.3885            5.94m\n",
      "      3000       11243.3212          -0.4788            5.21m\n",
      "      4000       11094.9605          -0.7175            4.47m\n",
      "      5000       10975.8694          -0.6824            3.72m\n",
      "      6000       10897.8457          -0.6577            2.98m\n",
      "      7000       10882.8941          -0.6871            2.23m\n",
      "      8000       10888.5545          -0.7335            1.49m\n",
      "      9000       10957.1246          -0.8885           44.70s\n",
      "     10000       10926.3024          -0.6595            0.00s\n",
      "train_loss: 0.629457, val_loss: 0.755212\ttime elapsed: 0:07:43\n",
      "accuracy score:  0.6831924577373212\n",
      "f score:  0.38202862805821786\n",
      "Running Fold 4 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13627.9198          22.6243            7.83m\n",
      "         2       13545.6328          20.8937            7.83m\n",
      "         3       13427.9571          20.1808            7.72m\n",
      "         4       13432.4668          18.7686            7.75m\n",
      "         5       13426.0550          18.3570            7.70m\n",
      "         6       13283.5473          18.2600            7.69m\n",
      "         7       13337.4334          16.8297            7.71m\n",
      "         8       13259.8016          15.2421            7.76m\n",
      "         9       13262.1800          14.7605            7.75m\n",
      "        10       13244.5962          13.3912            7.76m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        20       12922.6931           8.3530            7.67m\n",
      "        30       12798.4093           5.7352            7.65m\n",
      "        40       12635.6369           3.5996            7.62m\n",
      "        50       12481.9753           2.3650            7.60m\n",
      "        60       12432.1291           1.6707            7.58m\n",
      "        70       12370.7565           0.9938            7.57m\n",
      "        80       12310.5703           0.7301            7.55m\n",
      "        90       12387.9154           0.0537            7.56m\n",
      "       100       12216.4832           0.3325            7.55m\n",
      "       200       12116.4898          -0.1302            7.43m\n",
      "       300       12076.9726          -0.3748            7.32m\n",
      "       400       11970.8031          -0.3122            7.22m\n",
      "       500       11829.1061          -0.2682            7.13m\n",
      "       600       11846.4902          -0.5306            7.06m\n",
      "       700       11651.9959          -0.5259            6.99m\n",
      "       800       11729.3979          -0.7391            6.90m\n",
      "       900       11684.2848          -0.3378            6.82m\n",
      "      1000       11591.4447          -0.4845            6.74m\n",
      "      2000       11393.7043          -0.7491            5.99m\n",
      "      3000       11284.3426          -0.7601            5.24m\n",
      "      4000       11075.8298          -0.7442            4.49m\n",
      "      5000       11028.6239          -0.5918            3.74m\n",
      "      6000       10967.0991          -0.6004            2.99m\n",
      "      7000       10969.8746          -0.6753            2.25m\n",
      "      8000       10861.7526          -0.8869            1.50m\n",
      "      9000       10798.2025          -0.7263           45.04s\n",
      "     10000       10832.2194          -0.8515            0.00s\n",
      "train_loss: 0.628157, val_loss: 0.781175\ttime elapsed: 0:07:47\n",
      "accuracy score:  0.6782636969598439\n",
      "f score:  0.3833543476167378\n",
      "Running Fold 5 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13643.2639          22.1096            7.67m\n",
      "         2       13613.7949          21.5087            7.75m\n",
      "         3       13504.3975          20.1193            7.78m\n",
      "         4       13425.8112          18.2332            7.83m\n",
      "         5       13354.1549          17.1382            7.80m\n",
      "         6       13316.6047          16.8770            7.80m\n",
      "         7       13261.2064          16.5536            7.78m\n",
      "         8       13195.3166          15.4385            7.85m\n",
      "         9       13218.8229          14.5650            7.92m\n",
      "        10       13202.8129          14.4353            7.93m\n",
      "        20       12891.6210           8.5396            7.81m\n",
      "        30       12744.4704           5.5991            7.75m\n",
      "        40       12709.6952           3.8389            7.70m\n",
      "        50       12542.0625           1.9134            7.70m\n",
      "        60       12455.6548           1.0884            7.66m\n",
      "        70       12344.6091           1.1192            7.63m\n",
      "        80       12420.2816           0.5438            7.64m\n",
      "        90       12316.8347           0.4793            7.63m\n",
      "       100       12303.5951           0.1281            7.61m\n",
      "       200       12150.8193          -0.2112            7.42m\n",
      "       300       12063.2013          -0.2837            7.32m\n",
      "       400       11991.1277          -0.5108            7.24m\n",
      "       500       11883.6075          -0.4434            7.15m\n",
      "       600       11829.5540          -0.5540            7.06m\n",
      "       700       11715.9380          -0.4411            6.98m\n",
      "       800       11810.1264          -0.3348            6.89m\n",
      "       900       11707.5807          -0.4922            6.81m\n",
      "      1000       11722.9510          -0.6409            6.74m\n",
      "      2000       11394.9398          -0.6951            5.99m\n",
      "      3000       11183.8395          -0.7825            5.23m\n",
      "      4000       11159.2211          -0.6219            4.48m\n",
      "      5000       10942.9464          -0.7803            3.73m\n",
      "      6000       11013.3739          -0.7482            2.98m\n",
      "      7000       10900.1170          -0.6243            2.24m\n",
      "      8000       10977.9812          -0.6616            1.50m\n",
      "      9000       10921.7819          -0.4443           45.12s\n",
      "     10000       10868.8663          -0.7773            0.00s\n",
      "train_loss: 0.629639, val_loss: 0.776559\ttime elapsed: 0:07:48\n",
      "accuracy score:  0.686880182084214\n",
      "f score:  0.396404258092974\n",
      "train_loss mean: 0.628617, val_loss mean: 0.777421\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.02, loss='deviance', max_depth=5,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
       "                            n_iter_no_change=None, presort='deprecated',\n",
       "                            random_state=36683, subsample=0.7, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=1,\n",
       "                            warm_start=False),\n",
       " [[0.6286020148340137,\n",
       "   0.787605508697439,\n",
       "   0.37314603616017267,\n",
       "   0.6794538361508452],\n",
       "  [0.627229294863351,\n",
       "   0.7865556216002709,\n",
       "   0.38593561689761,\n",
       "   0.6840052015604682],\n",
       "  [0.6294565269737604,\n",
       "   0.7552119705960043,\n",
       "   0.38202862805821786,\n",
       "   0.6831924577373212],\n",
       "  [0.6281574551075673,\n",
       "   0.7811752763803589,\n",
       "   0.3833543476167378,\n",
       "   0.6782636969598439],\n",
       "  [0.6296392026434313,\n",
       "   0.7765585052524424,\n",
       "   0.396404258092974,\n",
       "   0.686880182084214]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "params = {'learning_rate': 0.02,\n",
    "          'n_estimators': 10000,\n",
    "          'subsample': 0.7,\n",
    "          'max_depth': 5,\n",
    "          'random_state': 36683,\n",
    "          'verbose': 1\n",
    "         }\n",
    "gb_clf.set_params(**params)\n",
    "train_cv(gb_clf, X, y, preprocess='no_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13581.4371          22.2174           46.96s\n",
      "         2       13578.5325          21.9653           46.91s\n",
      "         3       13515.1495          19.6330           47.19s\n",
      "         4       13473.0160          18.4145           47.56s\n",
      "         5       13321.8872          17.1375           47.56s\n",
      "         6       13387.3036          17.2491           47.38s\n",
      "         7       13253.6058          16.8130           47.24s\n",
      "         8       13268.9248          14.9846           47.00s\n",
      "         9       13269.1824          14.1551           47.13s\n",
      "        10       13166.7659          13.5459           47.02s\n",
      "        20       12958.7685           9.0291           46.70s\n",
      "        30       12726.2439           5.5933           45.62s\n",
      "        40       12682.3492           3.9573           45.26s\n",
      "        50       12584.3929           2.1353           44.59s\n",
      "        60       12525.8798           1.5672           43.88s\n",
      "        70       12460.0713           1.2228           43.47s\n",
      "        80       12405.3677           0.7387           42.94s\n",
      "        90       12272.7753           0.1824           42.44s\n",
      "       100       12279.8858           0.4090           41.81s\n",
      "       200       12051.9789          -0.0578           36.72s\n",
      "       300       12014.4958          -0.3468           31.90s\n",
      "       400       12006.4295          -0.3024           27.24s\n",
      "       500       11864.2791          -0.2122           22.70s\n",
      "       600       11811.8320          -0.4546           18.13s\n",
      "       700       11747.5234          -0.4156           13.59s\n",
      "       800       11578.9627          -0.6504            9.06s\n",
      "       900       11756.0340          -0.3578            4.53s\n",
      "      1000       11580.5217          -0.3723            0.00s\n",
      "train_loss: 0.675472, val_loss: 0.723301\ttime elapsed: 0:00:46\n",
      "accuracy score:  0.6911573472041612\n",
      "f score:  0.3549329288831719\n",
      "Running Fold 2 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13591.3643          22.4777           45.95s\n",
      "         2       13600.6381          19.6716           45.40s\n",
      "         3       13561.2576          18.5430           45.86s\n",
      "         4       13413.2628          18.9541           46.31s\n",
      "         5       13409.1401          17.1265           46.37s\n",
      "         6       13344.8268          15.3772           46.72s\n",
      "         7       13334.5801          15.0253           46.67s\n",
      "         8       13285.4924          14.5961           46.62s\n",
      "         9       13288.7802          14.2634           46.36s\n",
      "        10       13177.4906          13.4234           46.33s\n",
      "        20       12898.4697           7.7370           46.06s\n",
      "        30       12823.3834           5.2090           45.27s\n",
      "        40       12609.1666           3.6271           44.81s\n",
      "        50       12587.8809           2.1486           44.27s\n",
      "        60       12491.0411           1.7936           43.76s\n",
      "        70       12429.0952           1.0001           43.22s\n",
      "        80       12402.8717           0.6633           42.72s\n",
      "        90       12385.3892           0.6466           42.19s\n",
      "       100       12328.3980           0.1174           41.62s\n",
      "       200       12127.8779          -0.4435           36.72s\n",
      "       300       11986.4099          -0.4268           31.97s\n",
      "       400       11980.6438          -0.2787           27.35s\n",
      "       500       11957.7641          -0.4329           22.76s\n",
      "       600       11931.7216          -0.4729           18.19s\n",
      "       700       11811.7358          -0.2371           13.63s\n",
      "       800       11802.8856          -0.5424            9.08s\n",
      "       900       11653.0716          -0.4907            4.54s\n",
      "      1000       11660.1373          -0.4396            0.00s\n",
      "train_loss: 0.677845, val_loss: 0.717074\ttime elapsed: 0:00:46\n",
      "accuracy score:  0.6888816644993498\n",
      "f score:  0.3410952891077786\n",
      "Running Fold 3 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13629.3226          24.0740           47.96s\n",
      "         2       13530.9798          22.4596           46.91s\n",
      "         3       13521.1420          19.5316           47.52s\n",
      "         4       13465.2750          19.5319           48.06s\n",
      "         5       13367.1921          18.7148           48.36s\n",
      "         6       13376.5244          17.9918           48.54s\n",
      "         7       13332.4937          16.0336           48.37s\n",
      "         8       13299.9943          15.0615           48.48s\n",
      "         9       13282.8207          14.9046           48.12s\n",
      "        10       13220.0432          14.4838           48.21s\n",
      "        20       12922.4809           8.9187           46.65s\n",
      "        30       12738.2924           5.7975           45.62s\n",
      "        40       12520.5332           3.4817           45.22s\n",
      "        50       12505.2776           2.4583           44.57s\n",
      "        60       12432.9498           1.5397           43.99s\n",
      "        70       12481.5780           0.7261           43.56s\n",
      "        80       12299.3041           0.6213           43.01s\n",
      "        90       12381.5207           0.5146           42.44s\n",
      "       100       12314.0533           0.2727           41.87s\n",
      "       200       12183.0152          -0.2677           36.51s\n",
      "       300       12021.7910          -0.4608           31.85s\n",
      "       400       11913.3377          -0.1100           27.26s\n",
      "       500       11873.2530          -0.3140           22.71s\n",
      "       600       11767.8081          -0.3564           18.16s\n",
      "       700       11720.2601          -0.6387           13.63s\n",
      "       800       11635.8120          -0.5814            9.09s\n",
      "       900       11667.5209          -0.4212            4.54s\n",
      "      1000       11631.3780          -0.6161            0.00s\n",
      "train_loss: 0.672956, val_loss: 0.730861\ttime elapsed: 0:00:46\n",
      "accuracy score:  0.6874187256176854\n",
      "f score:  0.33907459576361076\n",
      "Running Fold 4 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13613.0120          22.6933           46.97s\n",
      "         2       13452.2021          22.3198           47.90s\n",
      "         3       13409.2176          20.6463           47.52s\n",
      "         4       13485.8492          18.7964           47.06s\n",
      "         5       13412.9878          17.9982           46.77s\n",
      "         6       13473.4556          16.9820           46.72s\n",
      "         7       13291.8773          16.5747           46.53s\n",
      "         8       13303.3528          15.9401           46.87s\n",
      "         9       13260.1713          14.9839           46.91s\n",
      "        10       13182.4157          13.9372           46.63s\n",
      "        20       12928.4412           8.9424           46.55s\n",
      "        30       12841.0038           5.8488           45.85s\n",
      "        40       12556.2095           3.5968           45.24s\n",
      "        50       12461.1574           2.1150           44.46s\n",
      "        60       12486.0178           1.6043           43.80s\n",
      "        70       12400.5772           1.0214           43.26s\n",
      "        80       12379.8381           0.6292           42.68s\n",
      "        90       12326.3593           0.4796           42.10s\n",
      "       100       12314.5313           0.2953           41.54s\n",
      "       200       12143.8338          -0.0504           36.57s\n",
      "       300       11939.7794          -0.3198           31.80s\n",
      "       400       11945.8451          -0.2955           27.23s\n",
      "       500       11805.1361          -0.3431           22.63s\n",
      "       600       11907.2155          -0.3554           18.09s\n",
      "       700       11741.6963          -0.4571           13.57s\n",
      "       800       11768.7993          -0.4572            9.05s\n",
      "       900       11649.4328          -0.4410            4.53s\n",
      "      1000       11569.3965          -0.6147            0.00s\n",
      "train_loss: 0.673823, val_loss: 0.729184\ttime elapsed: 0:00:46\n",
      "accuracy score:  0.6893188099496017\n",
      "f score:  0.3399189294503841\n",
      "Running Fold 5 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13577.5008          23.4632           48.95s\n",
      "         2       13555.3886          21.1697           47.41s\n",
      "         3       13512.0630          20.4152           47.52s\n",
      "         4       13397.2286          18.1393           47.31s\n",
      "         5       13325.0924          18.3460           46.97s\n",
      "         6       13355.0110          17.1923           46.72s\n",
      "         7       13410.9158          15.9431           46.67s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         8       13228.7285          15.2609           46.87s\n",
      "         9       13214.4378          14.2901           46.91s\n",
      "        10       13229.9286          14.1058           46.83s\n",
      "        20       12928.2068           8.9248           46.06s\n",
      "        30       12825.0995           5.4031           45.62s\n",
      "        40       12612.6755           3.7239           45.14s\n",
      "        50       12534.2144           2.4819           44.54s\n",
      "        60       12413.7606           1.3011           43.91s\n",
      "        70       12353.5873           0.9131           43.32s\n",
      "        80       12443.0439           0.6028           42.71s\n",
      "        90       12329.5988           0.1683           42.18s\n",
      "       100       12309.8133           0.3419           41.67s\n",
      "       200       12164.0962          -0.1586           36.64s\n",
      "       300       12034.5339          -0.3775           31.90s\n",
      "       400       12034.0795          -0.3783           27.35s\n",
      "       500       11906.1531          -0.4930           22.76s\n",
      "       600       11856.8528          -0.5495           18.18s\n",
      "       700       11744.5106          -0.3397           13.63s\n",
      "       800       11716.2993          -0.6212            9.08s\n",
      "       900       11751.7911          -0.6605            4.54s\n",
      "      1000       11690.3572          -0.4389            0.00s\n",
      "train_loss: 0.674956, val_loss: 0.721709\ttime elapsed: 0:00:46\n",
      "accuracy score:  0.6885059339944725\n",
      "f score:  0.3460565187580242\n",
      "train_loss mean: 0.675010, val_loss mean: 0.724426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.02, loss='deviance', max_depth=5,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                            n_iter_no_change=None, presort='deprecated',\n",
       "                            random_state=36683, subsample=0.7, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=1,\n",
       "                            warm_start=False),\n",
       " [[0.675471621982782,\n",
       "   0.7233011553639045,\n",
       "   0.3549329288831719,\n",
       "   0.6911573472041612],\n",
       "  [0.6778448981806487,\n",
       "   0.7170742489944126,\n",
       "   0.3410952891077786,\n",
       "   0.6888816644993498],\n",
       "  [0.6729561289880778,\n",
       "   0.7308608035195158,\n",
       "   0.33907459576361076,\n",
       "   0.6874187256176854],\n",
       "  [0.6738234012367011,\n",
       "   0.7291842720718774,\n",
       "   0.3399189294503841,\n",
       "   0.6893188099496017],\n",
       "  [0.6749558108204009,\n",
       "   0.7217093206505549,\n",
       "   0.3460565187580242,\n",
       "   0.6885059339944725]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "params = {'learning_rate': 0.02,\n",
    "          'n_estimators': 1000,\n",
    "          'subsample': 0.7,\n",
    "          'max_depth': 5,\n",
    "          'random_state': 36683,\n",
    "          'verbose': 1\n",
    "         }\n",
    "gb_clf.set_params(**params)\n",
    "train_cv(gb_clf, X, y, preprocess='no_preprocess')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Extra Trees Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "et_features = ['bathrooms', 'bedrooms', 'price', 'latitude', 'longitude']\n",
    "et_X = data[et_features]\n",
    "et_y = data['interest_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                     criterion='entropy', max_depth=7, max_features='sqrt',\n",
       "                     max_leaf_nodes=None, max_samples=None,\n",
       "                     min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                     min_samples_leaf=1, min_samples_split=2,\n",
       "                     min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "                     oob_score=True, random_state=36683, verbose=1,\n",
       "                     warm_start=False)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "et_clf = ExtraTreesClassifier()\n",
    "params = {'max_features': 'sqrt',\n",
    "              'criterion': 'entropy',\n",
    "              'max_depth': 7,\n",
    "              'n_estimators': 2000,\n",
    "              'bootstrap': True,\n",
    "              'oob_score': True,\n",
    "              'n_jobs': -1,\n",
    "              'verbose': 1,\n",
    "              'random_state': 36683\n",
    "              }\n",
    "et_clf.set_params(**params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:    4.2s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.730164, val_loss: 0.734653\ttime elapsed: 0:00:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.6890442132639792\n",
      "f score:  0.27386396266350677\n",
      "Running Fold 2 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.729583, val_loss: 0.734236\ttime elapsed: 0:00:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.6892067620286085\n",
      "f score:  0.2748679053394712\n",
      "Running Fold 3 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.8s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.9s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.727655, val_loss: 0.736722\ttime elapsed: 0:00:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.6s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.6s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.6890442132639792\n",
      "f score:  0.2734434566196251\n",
      "Running Fold 4 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    1.0s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.8s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:    4.0s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:    4.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.7s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.5s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.728810, val_loss: 0.735254\ttime elapsed: 0:00:11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.690131685904731\n",
      "f score:  0.2802296268524573\n",
      "Running Fold 5 / 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=-1)]: Done 168 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=-1)]: Done 418 tasks      | elapsed:    0.9s\n",
      "[Parallel(n_jobs=-1)]: Done 768 tasks      | elapsed:    1.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1218 tasks      | elapsed:    2.6s\n",
      "[Parallel(n_jobs=-1)]: Done 1768 tasks      | elapsed:    3.8s\n",
      "[Parallel(n_jobs=-1)]: Done 2000 out of 2000 | elapsed:    4.3s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.8s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.6s finished\n",
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss: 0.729481, val_loss: 0.735037\ttime elapsed: 0:00:10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    0.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy score:  0.6888310843765242\n",
      "f score:  0.2719424922178364\n",
      "train_loss mean: 0.729139, val_loss mean: 0.735180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(ExtraTreesClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                      criterion='entropy', max_depth=7, max_features='sqrt',\n",
       "                      max_leaf_nodes=None, max_samples=None,\n",
       "                      min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                      min_samples_leaf=1, min_samples_split=2,\n",
       "                      min_weight_fraction_leaf=0.0, n_estimators=2000, n_jobs=-1,\n",
       "                      oob_score=True, random_state=36683, verbose=1,\n",
       "                      warm_start=False),\n",
       " [[0.7301637694208001,\n",
       "   0.734653365146154,\n",
       "   0.27386396266350677,\n",
       "   0.6890442132639792],\n",
       "  [0.7295829883144259,\n",
       "   0.7342355482267169,\n",
       "   0.2748679053394712,\n",
       "   0.6892067620286085],\n",
       "  [0.7276553355347098,\n",
       "   0.7367217472665301,\n",
       "   0.2734434566196251,\n",
       "   0.6890442132639792],\n",
       "  [0.7288097356034714,\n",
       "   0.7352541411746049,\n",
       "   0.2802296268524573,\n",
       "   0.690131685904731],\n",
       "  [0.7294809979547392,\n",
       "   0.7350369682957243,\n",
       "   0.2719424922178364,\n",
       "   0.6888310843765242]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cv(et_clf, et_X, et_y, preprocess='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['bathrooms', 'bedrooms', 'latitude', 'longitude', 'price',\n",
       "       'num_of_photos', 'num_of_features', 'len_of_description',\n",
       "       'price_per_bedroom', 'price_per_bathroom', 'price_per_bed_bath_room',\n",
       "       'dist_to_city_center', 'kw_quiet_count', 'kw_new_count',\n",
       "       'kw_close_count', 'kw_spacious_count', 'kw_convinient_count',\n",
       "       'kw_safe_count', 'kw_care_count', 'pos_count'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# rf_features = ['price', 'hour', 'longtitude', 'latitude', 'dist_to_city_center','price_per_bedroom', 'price_per_bathroom', 'num_of_features', 'len_of_description', 'pos_count', 'num_of_photos', 'bedrooms', 'bathrooms', 'kw_quiet_count']\n",
    "rf_y = data['interest_level']\n",
    "# rf_X = data.loc[:, data.columns != 'interest_level'].copy()\n",
    "rf_X = data.drop(['index', 'listing_id', 'interest_level', 'hour'], axis=1).copy()\n",
    "# rf_X = data[rf_features]\n",
    "rf_features = rf_X.columns\n",
    "\n",
    "# rf_X\n",
    "rf_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold 1 / 5\n",
      "train_loss: 0.169320, val_loss: 0.672262\ttime elapsed: 0:00:27\n",
      "accuracy score:  0.7070871261378413\n",
      "f score:  0.48199723807407335\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.171599, val_loss: 0.655120\ttime elapsed: 0:00:26\n",
      "accuracy score:  0.7174902470741222\n",
      "f score:  0.5062340619687696\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.169470, val_loss: 0.675188\ttime elapsed: 0:00:25\n",
      "accuracy score:  0.711963589076723\n",
      "f score:  0.48464086669305634\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.170065, val_loss: 0.665025\ttime elapsed: 0:00:25\n",
      "accuracy score:  0.7146805397496342\n",
      "f score:  0.49613456784070903\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.171294, val_loss: 0.651890\ttime elapsed: 0:00:26\n",
      "accuracy score:  0.7179320435701512\n",
      "f score:  0.489306663871202\n",
      "train_loss mean: 0.170350, val_loss mean: 0.663897\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,\n",
       "                        criterion='entropy', max_depth=None, max_features='sqrt',\n",
       "                        max_leaf_nodes=None, max_samples=None,\n",
       "                        min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                        min_samples_leaf=1, min_samples_split=2,\n",
       "                        min_weight_fraction_leaf=0.0, n_estimators=2000,\n",
       "                        n_jobs=-1, oob_score=True, random_state=999, verbose=0,\n",
       "                        warm_start=False),\n",
       " [[0.16932017247128525,\n",
       "   0.6722622576793343,\n",
       "   0.48199723807407335,\n",
       "   0.7070871261378413],\n",
       "  [0.17159914564900708,\n",
       "   0.6551198807050362,\n",
       "   0.5062340619687696,\n",
       "   0.7174902470741222],\n",
       "  [0.16947009103406277,\n",
       "   0.6751882019733594,\n",
       "   0.48464086669305634,\n",
       "   0.711963589076723],\n",
       "  [0.17006549156800033,\n",
       "   0.6650246511429032,\n",
       "   0.49613456784070903,\n",
       "   0.7146805397496342],\n",
       "  [0.17129437858921898,\n",
       "   0.6518896510886163,\n",
       "   0.489306663871202,\n",
       "   0.7179320435701512]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {'max_features': 'sqrt',\n",
    "            'criterion': 'entropy',\n",
    "            'max_depth': None,\n",
    "            'n_estimators': 2000,\n",
    "            'oob_score': True,\n",
    "            'n_jobs': -1,\n",
    "            'random_state': 999\n",
    "            }\n",
    "rf_clf = RandomForestClassifier()\n",
    "rf_clf.set_params(**params)\n",
    "\n",
    "train_cv(rf_clf, rf_X, rf_y, preprocess='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMode(clf, which, features):\n",
    "    data = pd.read_csv('preprocessed.csv')\n",
    "    test_data = pd.read_csv('preprocessed_test_data.csv')\n",
    "\n",
    "    listing_id = test_data['listing_id']\n",
    "    # test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
    "    data, test_data = _preprocess(data, test_data)\n",
    "\n",
    "    # test_data.info()\n",
    "    test_data_x = test_data[features]\n",
    "\n",
    "    predictions = clf.predict_proba(test_data_x)\n",
    "    # test_data\n",
    "    predictions\n",
    "    # res\n",
    "    submission = pd.DataFrame({'listing_id':listing_id,'low':predictions[:,0], 'medium': predictions[:,1], 'high': predictions[:,2] })\n",
    "    submission.listing_id = submission.listing_id.astype(int)\n",
    "\n",
    "    submission.info()\n",
    "    filename = 'output_' + which + '.csv'\n",
    "    submission.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74659 entries, 0 to 74658\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   listing_id  74659 non-null  int32  \n",
      " 1   low         74659 non-null  float64\n",
      " 2   medium      74659 non-null  float64\n",
      " 3   high        74659 non-null  float64\n",
      "dtypes: float64(3), int32(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "testMode(rf_clf, 'rf', rf_features)\n",
    "# data[rf_features]\n",
    "# test_data = pd.read_csv('preprocessed_test_data.csv')\n",
    "\n",
    "# test_data[rf_features]\n",
    "# rf_features = ['price', 'hour', 'longtitude', 'latitude', 'dist_to_city_center','price_per_bedroom', 'price_per_bathroom', 'num_of_features', 'len_of_description', 'pos_count', 'num_of_photos', 'bedrooms', 'bathrooms', 'kw_quiet_count']\n",
    "# rf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=16)]: Using backend ThreadingBackend with 16 concurrent workers.\n",
      "[Parallel(n_jobs=16)]: Done  18 tasks      | elapsed:    0.0s\n",
      "[Parallel(n_jobs=16)]: Done 168 tasks      | elapsed:    0.2s\n",
      "[Parallel(n_jobs=16)]: Done 418 tasks      | elapsed:    0.7s\n",
      "[Parallel(n_jobs=16)]: Done 768 tasks      | elapsed:    1.4s\n",
      "[Parallel(n_jobs=16)]: Done 1218 tasks      | elapsed:    2.3s\n",
      "[Parallel(n_jobs=16)]: Done 1768 tasks      | elapsed:    3.4s\n",
      "[Parallel(n_jobs=16)]: Done 2000 out of 2000 | elapsed:    3.8s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74659 entries, 0 to 74658\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   listing_id  74659 non-null  int32  \n",
      " 1   low         74659 non-null  float64\n",
      " 2   medium      74659 non-null  float64\n",
      " 3   high        74659 non-null  float64\n",
      "dtypes: float64(3), int32(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "testMode(et_clf, 'et', et_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 74659 entries, 0 to 74658\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   listing_id  74659 non-null  int32  \n",
      " 1   low         74659 non-null  float64\n",
      " 2   medium      74659 non-null  float64\n",
      " 3   high        74659 non-null  float64\n",
      "dtypes: float64(3), int32(1)\n",
      "memory usage: 2.0 MB\n"
     ]
    }
   ],
   "source": [
    "testMode(gb_clf, 'gb', gb_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
