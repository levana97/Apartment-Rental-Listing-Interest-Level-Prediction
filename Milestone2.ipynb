{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import seaborn as sns\n",
    "import requests\n",
    "import os\n",
    "import sys\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn import metrics\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import re, string, time\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.calibration import CalibratedClassifierCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 41011 entries, 0 to 41010\n",
      "Data columns (total 24 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   index                    41011 non-null  int64  \n",
      " 1   bathrooms                41011 non-null  float64\n",
      " 2   bedrooms                 41011 non-null  int64  \n",
      " 3   latitude                 41011 non-null  float64\n",
      " 4   listing_id               41011 non-null  int64  \n",
      " 5   longitude                41011 non-null  float64\n",
      " 6   price                    41011 non-null  int64  \n",
      " 7   interest_level           41011 non-null  int64  \n",
      " 8   hour                     41011 non-null  int64  \n",
      " 9   num_of_photos            41011 non-null  int64  \n",
      " 10  num_of_features          41011 non-null  int64  \n",
      " 11  len_of_description       41011 non-null  int64  \n",
      " 12  price_per_bedroom        41011 non-null  float64\n",
      " 13  price_per_bathroom       41011 non-null  float64\n",
      " 14  price_per_bed_bath_room  41011 non-null  float64\n",
      " 15  dist_to_city_center      41011 non-null  float64\n",
      " 16  kw_quiet_count           41011 non-null  int64  \n",
      " 17  kw_new_count             41011 non-null  int64  \n",
      " 18  kw_close_count           41011 non-null  int64  \n",
      " 19  kw_spacious_count        41011 non-null  int64  \n",
      " 20  kw_convinient_count      41011 non-null  int64  \n",
      " 21  kw_safe_count            41011 non-null  int64  \n",
      " 22  kw_care_count            41011 non-null  int64  \n",
      " 23  pos_count                41011 non-null  int64  \n",
      "dtypes: float64(7), int64(17)\n",
      "memory usage: 7.5 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    # dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    # dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "# passing split_data according to features\n",
    "def train_cv(clf, model, split_data, preprocess = 'linear'):\n",
    "    X_train, X_test, y_train, y_test = split_data\n",
    "    print()\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    elif preprocess == 'linear':\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "    elif preprocess == 'no_preprocess': \n",
    "        #use original data\n",
    "        pass\n",
    "        \n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "   \n",
    "    \n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train.iloc[train_ind, :], X_train.iloc[val_ind, :]\n",
    "        train_y, val_y = y_train.iloc[train_ind], y_train.iloc[val_ind]\n",
    "        if model == 'svm':\n",
    "            clf, train_loss, val_loss = svm_run_model(svm_clf, (train_x, train_y), (val_x, val_y))\n",
    "        if model == 'lr':\n",
    "            clf, train_loss, val_loss = lr_run_model(lr_clf, (train_x, train_y), (val_x, val_y))\n",
    "        if model == 'dt':\n",
    "            clf, train_loss, val_loss = dt_run_model(dt_clf, (train_x, train_y), (val_x, val_y))\n",
    "        \n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss), end=\"\\t\")\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        y_pred = clf.predict(val_x)\n",
    "        accuracy_score = metrics.accuracy_score(val_y, y_pred)\n",
    "        f_score = metrics.f1_score(val_y, y_pred, average='macro')\n",
    "        cv_scores.append([train_loss, val_loss, f_score, accuracy_score])\n",
    "        \n",
    "        print(\"accuracy score: \", accuracy_score)\n",
    "        print(\"f score: \", f_score)\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    \n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))\n",
    "\n",
    "    return clf, cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_features = ['price','bedrooms','bathrooms','latitude','longitude']\n",
    "svm_x = data[svm_features]\n",
    "svm_y = data['interest_level']\n",
    "#svm_x_train,svm_x_test,svm_y_train,svm_y_test = train_test_split(svm_x,svm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_split_data():\n",
    "    return train_test_split(svm_x,svm_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def svm_run_model(clf, train_data,test_data=None):\n",
    "    # print(test_data)\n",
    "    if test_data:\n",
    "        clf.fit(train_data[0],train_data[1])\n",
    "        y_train_predict, y_test_predict = clf.predict_proba(train_data[0]), clf.predict_proba(test_data[0])\n",
    "        y_train_loss, y_test_loss = log_loss(train_data[1],y_train_predict), log_loss(test_data[1], y_test_predict)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(train_data[0],train_data[1])\n",
    "        y_train_predict = clf.predict_proba(train_data[0])\n",
    "        y_train_loss = log_loss(train_data[1], y_train_predict)\n",
    "        return clf, y_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 5\n",
      "train_loss: 0.730135, val_loss: 0.736924\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6875812743823146\n",
      "f score:  0.3226797247514512\n",
      "Running Fold 2 / 5\n",
      "train_loss: 0.732083, val_loss: 0.729340\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6898569570871261\n",
      "f score:  0.31893650919231514\n",
      "Running Fold 3 / 5\n",
      "train_loss: 0.731297, val_loss: 0.732260\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6874187256176854\n",
      "f score:  0.3186379682819483\n",
      "Running Fold 4 / 5\n",
      "train_loss: 0.731517, val_loss: 0.731499\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6904568362867827\n",
      "f score:  0.32442087219187976\n",
      "Running Fold 5 / 5\n",
      "train_loss: 0.732228, val_loss: 0.728817\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6919200130060152\n",
      "f score:  0.32963058305467136\n",
      "train_loss mean: 0.731452, val_loss mean: 0.731768\n"
     ]
    }
   ],
   "source": [
    "# init svm clf\n",
    "clf_init = LinearSVC()\n",
    "params = {'penalty': 'l2',\n",
    "        'loss': 'squared_hinge',\n",
    "        'C':0.01,     \n",
    "        'multi_class': 'ovr',\n",
    "        'fit_intercept': True,\n",
    "        'verbose': 0,\n",
    "        'random_state': 36683,\n",
    "        }\n",
    "clf_init.set_params(**params)\n",
    "svm_clf = CalibratedClassifierCV(clf_init)\n",
    "\n",
    "\n",
    "svm_clf, svm_cv_scores = train_cv(svm_clf, 'svm',svm_split_data(), preprocess='log')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_features = ['bathrooms', 'bedrooms', 'price']\n",
    "X = data[lr_features]\n",
    "y = data['interest_level']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    return train_test_split(X,y)\n",
    "\n",
    "def lr_run_model(clf, dtrain, dtest=None):\n",
    "    if dtest:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return clf, y_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 5\n",
      "[LibLinear]train_loss: 0.747840, val_loss: 0.742989\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6874187256176854\n",
      "f score:  0.30171406468256745\n",
      "Running Fold 2 / 5\n",
      "[LibLinear]train_loss: 0.747205, val_loss: 0.745384\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6849804941482445\n",
      "f score:  0.293253689658215\n",
      "Running Fold 3 / 5\n",
      "[LibLinear]train_loss: 0.746614, val_loss: 0.748528\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6833550065019506\n",
      "f score:  0.29386812125392264\n",
      "Running Fold 4 / 5\n",
      "[LibLinear]train_loss: 0.746491, val_loss: 0.748675\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.683628678263697\n",
      "f score:  0.29534517913800823\n",
      "Running Fold 5 / 5\n",
      "[LibLinear]train_loss: 0.746282, val_loss: 0.749422\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.684604129409852\n",
      "f score:  0.2942158851542486\n",
      "train_loss mean: 0.746887, val_loss mean: 0.746999\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                    multi_class='ovr', n_jobs=-1, penalty='l2',\n",
       "                    random_state=36883, solver='liblinear', tol=0.0001,\n",
       "                    verbose=1, warm_start=False),\n",
       " [[0.7478401467482448,\n",
       "   0.7429889607293084,\n",
       "   0.30171406468256745,\n",
       "   0.6874187256176854],\n",
       "  [0.7472050831900721,\n",
       "   0.745383594213056,\n",
       "   0.293253689658215,\n",
       "   0.6849804941482445],\n",
       "  [0.7466142808701104,\n",
       "   0.7485278008592905,\n",
       "   0.29386812125392264,\n",
       "   0.6833550065019506],\n",
       "  [0.7464913845162143,\n",
       "   0.7486749688696462,\n",
       "   0.29534517913800823,\n",
       "   0.683628678263697],\n",
       "  [0.7462818006740871,\n",
       "   0.7494218763193425,\n",
       "   0.2942158851542486,\n",
       "   0.684604129409852]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init logistic regression clf\n",
    "lr_clf = LogisticRegression()\n",
    "params = {'C': 0.01,\n",
    "        'solver': 'liblinear',\n",
    "        'multi_class': 'ovr',\n",
    "        'n_jobs': -1,\n",
    "        'verbose': 1,\n",
    "        'max_iter': 10000,\n",
    "        'random_state': 36883\n",
    "        }\n",
    "lr_clf.set_params(**params)\n",
    "\n",
    "train_cv(lr_clf, 'lr',split_data(), preprocess='no_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 5\n",
      "[LibLinear]train_loss: 0.749131, val_loss: 0.743787\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6911573472041612\n",
      "f score:  0.3007470447733701\n",
      "Running Fold 2 / 5\n",
      "[LibLinear]train_loss: 0.747826, val_loss: 0.748326\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6895318595578673\n",
      "f score:  0.2964330033562418\n",
      "Running Fold 3 / 5\n",
      "[LibLinear]train_loss: 0.747317, val_loss: 0.750478\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.688556566970091\n",
      "f score:  0.2918196748990775\n",
      "Running Fold 4 / 5\n",
      "[LibLinear]train_loss: 0.746857, val_loss: 0.752156\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6849292797919038\n",
      "f score:  0.2939744673641533\n",
      "Running Fold 5 / 5\n",
      "[LibLinear]train_loss: 0.748449, val_loss: 0.746282\ttime elapsed: 0:00:00\n",
      "accuracy score:  0.6870427572752398\n",
      "f score:  0.2962542253348985\n",
      "train_loss mean: 0.747916, val_loss mean: 0.748206\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(LogisticRegression(C=0.01, class_weight=None, dual=False, fit_intercept=True,\n",
       "                    intercept_scaling=1, l1_ratio=None, max_iter=10000,\n",
       "                    multi_class='ovr', n_jobs=-1, penalty='l2',\n",
       "                    random_state=36883, solver='liblinear', tol=0.0001,\n",
       "                    verbose=1, warm_start=False),\n",
       " [[0.749130960960146,\n",
       "   0.7437866895433026,\n",
       "   0.3007470447733701,\n",
       "   0.6911573472041612],\n",
       "  [0.7478263283649799,\n",
       "   0.7483261028053857,\n",
       "   0.2964330033562418,\n",
       "   0.6895318595578673],\n",
       "  [0.7473165596219083,\n",
       "   0.7504779722542395,\n",
       "   0.2918196748990775,\n",
       "   0.688556566970091],\n",
       "  [0.746856943335572,\n",
       "   0.7521558414603248,\n",
       "   0.2939744673641533,\n",
       "   0.6849292797919038],\n",
       "  [0.7484493266396346,\n",
       "   0.7462818838215943,\n",
       "   0.2962542253348985,\n",
       "   0.6870427572752398]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_cv(lr_clf, 'lr',split_data(), preprocess='log')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_clf, lr_cv_scores = train_cv(lr_clf, 'lr',split_data(), preprocess='linear')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Decision tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dt_split_data(x, y):\n",
    "    return train_test_split(x,y)\n",
    "\n",
    "def dt_run_model(clf, dtrain, dtest=None):\n",
    "    if dtest:\n",
    "#         tree.plot_tree(clf.fit(dtrain[0], dtrain[1]), )\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        # print(y_train_pred, y_test_pred)\n",
    "        \n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "#         tree.plot_tree(clf.fit(dtrain[0], dtrain[1]))\n",
    "        clf.fit(dtrain[0], dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        # print(y_train_pred, y_test_pred)\n",
    "        return clf, y_train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 5\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float32').",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d71567b8e383>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;31m# init decision tree clf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mdt_clf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'entropy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'auto'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mdt_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_cv_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrain_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'dt'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_split_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdt_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-4-a487a4f320f3>\u001b[0m in \u001b[0;36mtrain_cv\u001b[1;34m(clf, model, split_data, preprocess)\u001b[0m\n\u001b[0;32m     64\u001b[0m             \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr_run_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlr_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodel\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dt'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 66\u001b[1;33m             \u001b[0mclf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt_run_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdt_clf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtrain_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mval_x\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     68\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"train_loss: {0:.6f}, val_loss: {1:.6f}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\\t\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-13-58e0cddb8933>\u001b[0m in \u001b[0;36mdt_run_model\u001b[1;34m(clf, dtrain, dtest)\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdtest\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;31m#         tree.plot_tree(clf.fit(dtrain[0], dtrain[1]), )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m         \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0my_train_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m         \u001b[0my_train_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlog_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtest\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    875\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    876\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 877\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    878\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    879\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\tree\\_classes.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    147\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    148\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 149\u001b[1;33m             \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mDTYPE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maccept_sparse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"csc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    150\u001b[0m             \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mensure_2d\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0missparse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[0;32m    576\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    577\u001b[0m             _assert_all_finite(array,\n\u001b[1;32m--> 578\u001b[1;33m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[0;32m    579\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    580\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python37\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[1;34m(X, allow_nan, msg_dtype)\u001b[0m\n\u001b[0;32m     58\u001b[0m                     \u001b[0mmsg_err\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                     (type_err,\n\u001b[1;32m---> 60\u001b[1;33m                      msg_dtype if msg_dtype is not None else X.dtype)\n\u001b[0m\u001b[0;32m     61\u001b[0m             )\n\u001b[0;32m     62\u001b[0m     \u001b[1;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float32')."
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('preprocessed.csv')\n",
    "test_data = pd.read_csv('preprocessed_test_data.csv')\n",
    "\n",
    "dt_features = ['price', 'dist_to_city_center','price_per_bedroom', 'price_per_bathroom', 'num_of_features','len_of_description', 'pos_count', 'num_of_photos', 'bedrooms', 'bathrooms']\n",
    "dt_x = data[dt_features]\n",
    "dt_y = data['interest_level']\n",
    "\n",
    "# init decision tree clf\n",
    "dt_clf = DecisionTreeClassifier(max_depth=4, criterion='entropy', max_features='auto')\n",
    "dt_clf, dt_cv_scores = train_cv(dt_clf, 'dt', dt_split_data(dt_x, dt_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test model with test.json and build submittion.csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testMode(clf, which, features):\n",
    "    data = pd.read_csv('preprocessed.csv')\n",
    "    test_data = pd.read_csv('preprocessed_test_data.csv')\n",
    "\n",
    "    listing_id = test_data['listing_id']\n",
    "    # test_data = test_data.replace([np.inf, -np.inf], np.nan)\n",
    "    data, test_data = _preprocess(data, test_data)\n",
    "\n",
    "    # test_data.info()\n",
    "    test_data_x = test_data[features]\n",
    "\n",
    "    predictions = clf.predict_proba(test_data_x)\n",
    "    # test_data\n",
    "    predictions\n",
    "    # res\n",
    "    submission = pd.DataFrame({'listing_id':listing_id,'low':predictions[:,0], 'medium': predictions[:,1], 'high': predictions[:,2] })\n",
    "    submission.listing_id = submission.listing_id.astype(int)\n",
    "\n",
    "    submission.info()\n",
    "    filename = 'output_' + which + '.csv'\n",
    "    submission.to_csv(filename,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMode(dt_clf, 'dt', dt_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMode(svm_clf, 'svm', svm_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testMode(lr_clf, 'lr', lr_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = ['train_loss', 'val_loss', 'f_score', 'accuracy_score']\n",
    "lr_scores = pd.DataFrame(lr_cv_scores, columns=cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_scores[cols].plot(title='Logistic Regression log_loss vs f_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_scores = pd.DataFrame(svm_cv_scores, columns=cols)\n",
    "svm_scores[cols].plot(title='SVM log_loss vs f_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_scores = pd.DataFrame(dt_cv_scores, columns=cols)\n",
    "dt_scores[cols].plot(title='Decision Tree log_loss vs f_score')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1rc2"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
