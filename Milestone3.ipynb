{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re, string, time\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn import metrics\n",
    "from sklearn.ensemble import GradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('preprocessed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Common Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _preprocess(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    # dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    # dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "\n",
    "def _preprocess_log(dtrain, dtest):\n",
    "    # replace np.inf to np.nan\n",
    "    dtrain = dtrain.replace([np.inf, -np.inf], np.nan)\n",
    "    dtest = dtest.replace([np.inf, -np.inf], np.nan)\n",
    "\n",
    "    # impute np.nan\n",
    "    dtrain_col_mean = dtrain.mean(axis=0)\n",
    "    dtrain, dtest = dtrain.fillna(dtrain_col_mean), dtest.fillna(dtrain_col_mean)\n",
    "\n",
    "    # log transform of min-zero columns\n",
    "    dtrain_col_min = dtrain.min(axis=0)\n",
    "    zero_min_index = dtrain_col_min[dtrain_col_min >= 0].index\n",
    "\n",
    "    dtrain[zero_min_index] = np.log10(dtrain[zero_min_index] + 1.0)\n",
    "    dtest[zero_min_index] = np.log10(dtest[zero_min_index] + 1.0)\n",
    "\n",
    "    # perform standardization\n",
    "    dtrain_col_mean, dtrain_col_std = dtrain.mean(axis=0), dtrain.std(axis=0)\n",
    "    dtrain, dtest = map(lambda x: (x - dtrain_col_mean) / dtrain_col_std, (dtrain, dtest))\n",
    "\n",
    "    return dtrain, dtest\n",
    "\n",
    "# passing split_data according to features\n",
    "def train_cv(clf, model, split_data, preprocess = 'linear'):\n",
    "    X_train, X_test, y_train, y_test = split_data\n",
    "    print()\n",
    "    if preprocess == 'log':\n",
    "        X_train, X_test = _preprocess_log(X_train, X_test)\n",
    "    elif preprocess == 'linear':\n",
    "        X_train, X_test = _preprocess(X_train, X_test)\n",
    "    elif preprocess == 'no_preprocess': \n",
    "        #use original data\n",
    "        pass\n",
    "        \n",
    "    cv_scores, n_folds = [], 5\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=816)\n",
    "   \n",
    "    \n",
    "    for i, (train_ind, val_ind) in enumerate(skf.split(X_train, y_train)):\n",
    "        print(\"Running Fold\", i + 1, \"/\", n_folds)\n",
    "        start = time.time()\n",
    "        \n",
    "        train_x, val_x = X_train.iloc[train_ind, :], X_train.iloc[val_ind, :]\n",
    "        train_y, val_y = y_train.iloc[train_ind], y_train.iloc[val_ind]\n",
    "        if model == 'svm':\n",
    "            clf, train_loss, val_loss = svm_run_model(svm_clf, (train_x, train_y), (val_x, val_y))\n",
    "        if model == 'gb':\n",
    "            clf, train_loss, val_loss = gb_run_model(gb_clf, (train_x, train_y), (val_x, val_y))\n",
    "        if model == 'dt':\n",
    "            clf, train_loss, val_loss = dt_run_model(dt_clf, (train_x, train_y), (val_x, val_y))\n",
    "        \n",
    "        print(\"train_loss: {0:.6f}, val_loss: {1:.6f}\".format(train_loss, val_loss), end=\"\\t\")\n",
    "        \n",
    "        end = time.time()\n",
    "        m, s = divmod(end-start, 60)\n",
    "        h, m = divmod(m, 60)\n",
    "\n",
    "        print(\"time elapsed: %d:%02d:%02d\" % (h, m, s))\n",
    "        y_pred = clf.predict(val_x)\n",
    "        accuracy_score = metrics.accuracy_score(val_y, y_pred)\n",
    "        f_score = metrics.f1_score(val_y, y_pred, average='macro')\n",
    "        cv_scores.append([train_loss, val_loss, f_score, accuracy_score])\n",
    "        \n",
    "        print(\"accuracy score: \", accuracy_score)\n",
    "        print(\"f score: \", f_score)\n",
    "        \n",
    "    mean_train_loss = np.mean([cv_scores[i][0] for i in range(len(cv_scores))])\n",
    "    mean_val_loss = np.mean([cv_scores[i][1] for i in range(len(cv_scores))])\n",
    "    \n",
    "    print(\"train_loss mean: {0:.6f}, val_loss mean: {1:.6f}\".format(mean_train_loss, mean_val_loss))\n",
    "\n",
    "    return clf, cv_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_features = ['bathrooms', 'bedrooms', 'price']\n",
    "X = data[gb_features]\n",
    "y = data['interest_level']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    return train_test_split(X,y)\n",
    "\n",
    "def gb_run_model(clf, dtrain,dtest=None):\n",
    "    if dtest:\n",
    "        clf.fit(dtrain[0],dtrain[1])\n",
    "        y_train_pred, y_test_pred = clf.predict_proba(dtrain[0]), clf.predict_proba(dtest[0])\n",
    "        y_train_loss, y_test_loss = log_loss(dtrain[1], y_train_pred), log_loss(dtest[1], y_test_pred)\n",
    "        return clf, y_train_loss, y_test_loss\n",
    "    else:\n",
    "        clf.fit(dtrain[0],dtrain[1])\n",
    "        y_train_pred = clf.predict_proba(dtrain[0])\n",
    "        y_train_loss = log_loss(dtrain[1], y_train_pred)\n",
    "        return clf, y_train_loss\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13611.3189          23.1097            8.79m\n",
      "         2       13550.2589          21.5501            8.52m\n",
      "         3       13522.1604          19.6519            8.43m\n",
      "         4       13471.6136          19.6575            9.33m\n",
      "         5       13427.4461          18.5323            9.09m\n",
      "         6       13487.4414          16.1698            8.94m\n",
      "         7       13361.7994          16.2581            8.86m\n",
      "         8       13276.5114          15.2057            8.75m\n",
      "         9       13241.8961          14.6790            9.05m\n",
      "        10       13309.2943          13.7209            8.98m\n",
      "        20       12865.4916           8.9130            8.72m\n",
      "        30       12797.6860           5.1751            9.43m\n",
      "        40       12652.9192           3.7426            9.27m\n",
      "        50       12527.9846           2.2455            9.17m\n",
      "        60       12481.5589           1.4714            9.08m\n",
      "        70       12423.9847           0.9159            8.98m\n",
      "        80       12285.2926           0.3981            9.41m\n",
      "        90       12379.9022           0.1446            9.58m\n",
      "       100       12372.0249           0.1437            9.43m\n",
      "       200       12169.3677          -0.2918            8.42m\n",
      "       300       12143.4920          -0.2882            7.96m\n",
      "       400       11952.1744          -0.3381            7.90m\n",
      "       500       11997.4243          -0.6198            7.96m\n",
      "       600       11903.0243          -0.3142            7.81m\n",
      "       700       11823.3432          -0.4458            7.63m\n",
      "       800       11811.1899          -0.6252            7.55m\n",
      "       900       11670.1155          -0.5717            7.42m\n",
      "      1000       11642.1535          -0.8515            7.32m\n",
      "      2000       11322.3067          -0.6371            6.58m\n",
      "      3000       11315.8227          -0.7346            5.62m\n",
      "      4000       11171.4483          -0.6546            4.82m\n",
      "      5000       10955.8890          -0.8147            4.06m\n",
      "      6000       10902.9655          -0.9050            3.21m\n",
      "      7000       10886.4375          -0.8395            2.41m\n",
      "      8000       10949.8939          -0.7277            1.59m\n",
      "      9000       10940.2998          -0.6010           47.52s\n",
      "     10000       10893.8233          -0.8431            0.00s\n",
      "train_loss: 0.629846, val_loss: 0.780094\ttime elapsed: 0:08:11\n",
      "accuracy score:  0.6838940354298716\n",
      "f score:  0.3794254654982827\n",
      "Running Fold 2 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13601.7396          20.8287            8.20m\n",
      "         2       13575.9484          20.5588            8.12m\n",
      "         3       13515.0061          18.8863            8.28m\n",
      "         4       13488.5095          18.1683            8.80m\n",
      "         5       13421.3201          17.6999            9.68m\n",
      "         6       13344.4862          16.2202           12.62m\n",
      "         7       13453.1479          14.9570           13.97m\n",
      "         8       13218.0988          14.3699           13.52m\n",
      "         9       13287.8871          13.8060           14.32m\n",
      "        10       13247.3353          12.9038           14.79m\n",
      "        20       12983.8950           8.1745           14.93m\n",
      "        30       12799.1304           5.2543           12.90m\n",
      "        40       12667.4810           3.2764           11.82m\n",
      "        50       12546.6441           2.0249           11.20m\n",
      "        60       12553.6642           1.6011           10.75m\n",
      "        70       12434.3510           0.8442           10.41m\n",
      "        80       12521.2657           0.6720           10.16m\n",
      "        90       12394.3871           0.3750            9.94m\n",
      "       100       12377.2192           0.3435           10.05m\n",
      "       200       12245.3622          -0.1952            8.74m\n",
      "       300       12157.4479          -0.2234            8.42m\n",
      "       400       12095.8438          -0.4098            8.26m\n",
      "       500       11991.2730          -0.6124            7.95m\n",
      "       600       11922.7343          -0.3963            7.73m\n",
      "       700       11878.9924          -0.4327            7.55m\n",
      "       800       11879.5422          -0.5185            7.40m\n",
      "       900       11795.9157          -0.5869            7.28m\n",
      "      1000       11753.5659          -0.5504            7.15m\n",
      "      2000       11447.9088          -0.4865            6.49m\n",
      "      3000       11296.0484          -0.6899            5.54m\n",
      "      4000       11273.1574          -0.5426            4.72m\n",
      "      5000       11006.1038          -0.8251            3.91m\n",
      "      6000       11135.7345          -0.8611            3.14m\n",
      "      7000       10981.9081          -0.5747            2.38m\n",
      "      8000       10833.4491          -0.6680            1.58m\n",
      "      9000       10934.1100          -0.6754           47.14s\n",
      "     10000       10883.2035          -0.9058            0.00s\n",
      "train_loss: 0.631127, val_loss: 0.767326\ttime elapsed: 0:08:08\n",
      "accuracy score:  0.6810793237971391\n",
      "f score:  0.3733686973640262\n",
      "Running Fold 3 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13594.8634          21.8796            8.34m\n",
      "         2       13582.3673          20.4425            8.58m\n",
      "         3       13565.6308          19.7914            9.22m\n",
      "         4       13524.1300          18.3933           11.52m\n",
      "         5       13414.5784          17.2529           11.06m\n",
      "         6       13380.6477          16.8157           11.02m\n",
      "         7       13372.1044          15.5432           10.97m\n",
      "         8       13207.5092          16.5057           10.67m\n",
      "         9       13212.7518          14.7015           10.43m\n",
      "        10       13146.1518          13.9370           10.28m\n",
      "        20       12963.0140           8.7762            9.90m\n",
      "        30       12837.2077           5.3515           10.02m\n",
      "        40       12596.1404           3.4050           10.30m\n",
      "        50       12595.9407           2.3377           10.43m\n",
      "        60       12521.2247           1.4537           10.67m\n",
      "        70       12369.3314           0.6900           10.50m\n",
      "        80       12393.9357           0.5474           10.22m\n",
      "        90       12428.2478           0.6461           10.02m\n",
      "       100       12365.7627           0.1900            9.84m\n",
      "       200       12249.5470          -0.3473            9.15m\n",
      "       300       12159.7638          -0.2656            8.67m\n",
      "       400       11938.1835          -0.5011            8.35m\n",
      "       500       11895.0013          -0.4333            8.50m\n",
      "       600       11840.1414          -0.3662            8.28m\n",
      "       700       11765.2855          -0.5691            8.12m\n",
      "       800       11746.4225          -0.5346            7.99m\n",
      "       900       11730.6897          -0.5431            7.93m\n",
      "      1000       11614.5066          -0.4906            7.76m\n",
      "      2000       11329.6406          -0.7508            6.55m\n",
      "      3000       11243.1491          -0.5976            5.58m\n",
      "      4000       11153.9477          -1.0961            4.73m\n",
      "      5000       11090.7457          -0.7714            3.92m\n",
      "      6000       10908.6716          -0.7636            3.12m\n",
      "      7000       10930.1929          -0.9052            2.33m\n",
      "      8000       10922.6859          -0.9454            1.55m\n",
      "      9000       10853.2677          -0.6407           46.51s\n",
      "     10000       10863.5342          -0.9277            0.00s\n",
      "train_loss: 0.628454, val_loss: 0.801930\ttime elapsed: 0:08:03\n",
      "accuracy score:  0.6781534460338101\n",
      "f score:  0.38757689609610213\n",
      "Running Fold 4 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13634.6427          22.5856           10.75m\n",
      "         2       13574.4878          20.7733            9.50m\n",
      "         3       13580.8513          19.5575            9.07m\n",
      "         4       13452.3416          18.8656            9.45m\n",
      "         5       13424.5640          17.3296            9.21m\n",
      "         6       13393.4594          16.5235            9.05m\n",
      "         7       13337.1696          16.8459            8.94m\n",
      "         8       13373.0617          15.2540            9.20m\n",
      "         9       13284.3593          14.2726            9.09m\n",
      "        10       13249.1143          13.5265            9.00m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        20       12927.7901           8.5135            8.69m\n",
      "        30       12747.1858           5.4830            8.70m\n",
      "        40       12642.0248           3.3442            8.62m\n",
      "        50       12551.6003           2.2713            8.55m\n",
      "        60       12511.8724           1.2297            8.48m\n",
      "        70       12411.3023           1.0342            8.41m\n",
      "        80       12453.1428           0.5845            8.36m\n",
      "        90       12382.6727           0.3765            8.32m\n",
      "       100       12346.1899           0.0296            8.27m\n",
      "       200       12167.3438          -0.1436            7.87m\n",
      "       300       12034.5779          -0.4530            7.95m\n",
      "       400       12005.9184          -0.3604            7.69m\n",
      "       500       11929.8609          -0.3080            7.50m\n",
      "       600       11846.8161          -0.3698            7.38m\n",
      "       700       11846.2198          -0.3804            7.29m\n",
      "       800       11732.5062          -0.5321            7.18m\n",
      "       900       11739.4372          -0.4751            7.07m\n",
      "      1000       11626.0647          -0.6400            6.98m\n",
      "      2000       11393.2057          -0.6481            6.12m\n",
      "      3000       11241.4798          -0.9164            5.34m\n",
      "      4000       11117.3620          -0.8073            4.57m\n",
      "      5000       10988.7257          -0.6691            3.80m\n",
      "      6000       10938.9040          -0.6618            3.05m\n",
      "      7000       10963.4730          -0.8430            2.29m\n",
      "      8000       10862.9302          -0.7608            1.53m\n",
      "      9000       10836.7775          -0.8737           45.79s\n",
      "     10000       10847.2463          -0.9048            0.00s\n",
      "train_loss: 0.627511, val_loss: 0.794035\ttime elapsed: 0:07:56\n",
      "accuracy score:  0.6797268736790766\n",
      "f score:  0.38190825383620547\n",
      "Running Fold 5 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13550.1753          21.5632            8.32m\n",
      "         2       13563.4533          20.3250            8.20m\n",
      "         3       13573.1837          19.6762            8.35m\n",
      "         4       13569.1225          19.0638            8.58m\n",
      "         5       13441.4379          17.4670            8.52m\n",
      "         6       13368.4261          16.6803            8.45m\n",
      "         7       13372.1018          14.5827            8.42m\n",
      "         8       13342.4390          15.2660            8.56m\n",
      "         9       13248.4532          14.1180            8.51m\n",
      "        10       13285.9490          12.9778            8.47m\n",
      "        20       12978.7431           8.3441            8.31m\n",
      "        30       12799.3253           5.2715            8.36m\n",
      "        40       12687.1841           3.4363            8.35m\n",
      "        50       12633.1113           2.1865            8.78m\n",
      "        60       12541.5783           1.5764            9.03m\n",
      "        70       12506.9402           0.9891            8.95m\n",
      "        80       12474.5834           0.6675            8.83m\n",
      "        90       12444.8150           0.6495            8.73m\n",
      "       100       12360.4436           0.3581            8.67m\n",
      "       200       12176.2901          -0.5589            8.20m\n",
      "       300       12105.9799          -0.2866            7.83m\n",
      "       400       11968.0152          -0.3370            7.60m\n",
      "       500       11988.5062          -0.3707            7.42m\n",
      "       600       11919.1565          -0.5281            7.31m\n",
      "       700       11831.7705          -0.4706            7.19m\n",
      "       800       11742.4284          -0.4379            7.08m\n",
      "       900       11767.1900          -0.4307            6.99m\n",
      "      1000       11736.8870          -0.5592            6.90m\n",
      "      2000       11425.0313          -0.5666            6.06m\n",
      "      3000       11312.2123          -0.7357            5.28m\n",
      "      4000       11157.0462          -0.8184            4.52m\n",
      "      5000       11092.6745          -0.7570            3.77m\n",
      "      6000       10986.8350          -0.8202            3.01m\n",
      "      7000       11024.5922          -0.6809            2.26m\n",
      "      8000       10900.9805          -0.8264            1.51m\n",
      "      9000       10948.7259          -0.6968           45.21s\n",
      "     10000       10812.8755          -0.8323            0.00s\n",
      "train_loss: 0.630499, val_loss: 0.774145\ttime elapsed: 0:07:51\n",
      "accuracy score:  0.68\n",
      "f score:  0.3659079246035768\n",
      "train_loss mean: 0.629487, val_loss mean: 0.783506\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.02, loss='deviance', max_depth=5,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=10000,\n",
       "                            n_iter_no_change=None, presort='auto',\n",
       "                            random_state=36683, subsample=0.7, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=1,\n",
       "                            warm_start=False),\n",
       " [[0.6298458045002828,\n",
       "   0.7800941202854448,\n",
       "   0.3794254654982827,\n",
       "   0.6838940354298716],\n",
       "  [0.6311265644623326,\n",
       "   0.7673263807715758,\n",
       "   0.3733686973640262,\n",
       "   0.6810793237971391],\n",
       "  [0.6284543967895811,\n",
       "   0.8019302014145712,\n",
       "   0.38757689609610213,\n",
       "   0.6781534460338101],\n",
       "  [0.6275106673162203,\n",
       "   0.7940353669594064,\n",
       "   0.38190825383620547,\n",
       "   0.6797268736790766],\n",
       "  [0.6304989376956741, 0.7741446280082462, 0.3659079246035768, 0.68]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "params = {'learning_rate': 0.02,\n",
    "          'n_estimators': 10000,\n",
    "          'subsample': 0.7,\n",
    "          'max_depth': 5,\n",
    "          'random_state': 36683,\n",
    "          'verbose': 1\n",
    "         }\n",
    "gb_clf.set_params(**params)\n",
    "train_cv(gb_clf, 'gb',split_data(), preprocess='no_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Running Fold 1 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13564.7505          21.9900            1.13m\n",
      "         2       13504.5402          21.2265            1.04m\n",
      "         3       13377.6188          19.7885            1.08m\n",
      "         4       13401.6395          19.4041            1.07m\n",
      "         5       13387.5332          17.5449            1.02m\n",
      "         6       13351.1424          16.2451           59.13s\n",
      "         7       13215.9076          15.8187           57.91s\n",
      "         8       13225.5754          15.8074           59.00s\n",
      "         9       13248.1530          14.9355           58.01s\n",
      "        10       13092.9963          14.2717           57.02s\n",
      "        20       12889.0398           8.1924           53.40s\n",
      "        30       12734.7514           5.1857           53.18s\n",
      "        40       12571.1657           2.9650           52.02s\n",
      "        50       12423.4573           2.4009           51.10s\n",
      "        60       12541.6083           1.3878           50.29s\n",
      "        70       12314.7213           0.6798           49.41s\n",
      "        80       12378.0724           0.7557           48.58s\n",
      "        90       12267.3474           0.2437           47.81s\n",
      "       100       12408.5724           0.2386           47.14s\n",
      "       200       11991.6433          -0.2119           39.34s\n",
      "       300       11961.5102          -0.3692           34.25s\n",
      "       400       11938.2067          -0.2587           29.41s\n",
      "       500       11755.7738          -0.5228           24.20s\n",
      "       600       11742.5572          -0.6752           19.15s\n",
      "       700       11764.8045          -0.6259           14.27s\n",
      "       800       11610.7842          -0.6162            9.47s\n",
      "       900       11592.7149          -0.6805            4.73s\n",
      "      1000       11598.5876          -0.4930            0.00s\n",
      "train_loss: 0.671904, val_loss: 0.722855\ttime elapsed: 0:00:49\n",
      "accuracy score:  0.6864943929790346\n",
      "f score:  0.3485588682695016\n",
      "Running Fold 2 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13646.7733          22.9219           58.98s\n",
      "         2       13469.2431          21.0882           54.08s\n",
      "         3       13383.6705          20.1173           52.70s\n",
      "         4       13428.3835          18.0751           56.52s\n",
      "         5       13338.1389          18.5465           55.46s\n",
      "         6       13391.0305          16.7048           54.62s\n",
      "         7       13344.8691          14.8894           55.53s\n",
      "         8       13251.0655          15.3106           54.81s\n",
      "         9       13282.6492          14.3871           55.30s\n",
      "        10       13170.3436          13.7772           54.61s\n",
      "        20       12861.6535           8.2774           51.86s\n",
      "        30       12652.5875           5.6151           51.41s\n",
      "        40       12590.9930           3.1178           50.67s\n",
      "        50       12485.4313           2.4308           51.16s\n",
      "        60       12320.8955           1.0657           50.54s\n",
      "        70       12391.0010           0.8070           49.68s\n",
      "        80       12255.6475           0.8064           48.75s\n",
      "        90       12329.0815           0.4534           48.26s\n",
      "       100       12266.6957           0.2899           47.44s\n",
      "       200       12096.2384          -0.2770           39.43s\n",
      "       300       12005.5936          -0.1836           33.67s\n",
      "       400       11875.7946          -0.3736           28.38s\n",
      "       500       11770.8010          -0.5651           23.43s\n",
      "       600       11754.4193          -0.4696           18.90s\n",
      "       700       11644.1555          -0.3472           14.15s\n",
      "       800       11591.0884          -0.5750            9.40s\n",
      "       900       11585.0485          -0.5832            4.69s\n",
      "      1000       11546.4126          -0.5423            0.00s\n",
      "train_loss: 0.670457, val_loss: 0.723936\ttime elapsed: 0:00:48\n",
      "accuracy score:  0.6918075422626788\n",
      "f score:  0.3561916920726081\n",
      "Running Fold 3 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13509.7157          21.8312           57.59s\n",
      "         2       13555.4874          20.8681           58.48s\n",
      "         3       13530.8765          19.5682           55.13s\n",
      "         4       13377.4490          18.6955           56.83s\n",
      "         5       13411.6585          17.8772           55.66s\n",
      "         6       13386.2878          15.7811           54.55s\n",
      "         7       13275.4546          15.5487           53.80s\n",
      "         8       13265.3038          14.5295           53.03s\n",
      "         9       13220.5493          14.5307           53.73s\n",
      "        10       13156.2514          13.5469           53.14s\n",
      "        20       12838.4188           8.9177           50.97s\n",
      "        30       12720.2012           5.3617           50.93s\n",
      "        40       12650.8321           3.5269           49.95s\n",
      "        50       12457.8496           1.6895           49.26s\n",
      "        60       12523.4430           1.3423           48.78s\n",
      "        70       12392.2435           0.8524           48.00s\n",
      "        80       12272.5436           0.2973           47.25s\n",
      "        90       12227.9556           0.4124           46.64s\n",
      "       100       12166.1650           0.2466           45.91s\n",
      "       200       12039.8289          -0.1990           38.68s\n",
      "       300       12071.5939          -0.4594           33.05s\n",
      "       400       11898.7687          -0.5156           27.93s\n",
      "       500       11853.0082          -0.3991           23.09s\n",
      "       600       11841.1792          -0.3692           18.50s\n",
      "       700       11736.2105          -0.5205           13.89s\n",
      "       800       11705.3535          -0.5408            9.26s\n",
      "       900       11566.2323          -0.6213            4.63s\n",
      "      1000       11658.8959          -0.5787            0.00s\n",
      "train_loss: 0.673362, val_loss: 0.719376\ttime elapsed: 0:00:48\n",
      "accuracy score:  0.6919200130060152\n",
      "f score:  0.35255309417823316\n",
      "Running Fold 4 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13612.1998          22.1616            1.05m\n",
      "         2       13315.4831          20.7804           56.19s\n",
      "         3       13485.6236          19.6646           53.94s\n",
      "         4       13399.1278          18.5792           58.40s\n",
      "         5       13408.7158          17.8064           56.47s\n",
      "         6       13270.2755          16.6309           55.21s\n",
      "         7       13235.9907          14.9909           54.24s\n",
      "         8       13194.3518          14.8160           53.60s\n",
      "         9       13190.2952          13.9769           55.64s\n",
      "        10       13164.8540          14.3231           55.26s\n",
      "        20       12917.3598           8.4226           53.13s\n",
      "        30       12717.3097           5.6306           52.12s\n",
      "        40       12567.0972           3.6297           51.10s\n",
      "        50       12454.0656           2.5092           50.22s\n",
      "        60       12389.9685           1.4900           49.71s\n",
      "        70       12356.7161           0.7751           48.82s\n",
      "        80       12311.2648           0.5442           47.93s\n",
      "        90       12287.4024           0.3392           47.04s\n",
      "       100       12271.1780           0.3094           46.19s\n",
      "       200       12133.8264          -0.3172           40.39s\n",
      "       300       11981.4865          -0.1915           34.74s\n",
      "       400       11908.0231          -0.2089           29.11s\n",
      "       500       11832.7512          -0.3543           23.93s\n",
      "       600       11773.8545          -0.4246           19.03s\n",
      "       700       11627.2088          -0.5885           14.19s\n",
      "       800       11639.7430          -0.4786            9.43s\n",
      "       900       11551.5090          -0.6192            4.71s\n",
      "      1000       11607.5027          -0.4744            0.00s\n",
      "train_loss: 0.671220, val_loss: 0.725678\ttime elapsed: 0:00:48\n",
      "accuracy score:  0.6870427572752398\n",
      "f score:  0.3362617122855598\n",
      "Running Fold 5 / 5\n",
      "      Iter       Train Loss      OOB Improve   Remaining Time \n",
      "         1       13552.4859          21.3506           58.07s\n",
      "         2       13442.8584          19.8032           53.31s\n",
      "         3       13373.4983          21.1832           51.84s\n",
      "         4       13412.8591          18.2215           51.37s\n",
      "         5       13410.1856          17.1190           53.63s\n",
      "         6       13335.7962          16.6253           52.98s\n",
      "         7       13323.1806          14.7580           52.32s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         8       13178.2271          15.3701           52.04s\n",
      "         9       13259.4540          14.0742           53.20s\n",
      "        10       13218.8273          12.9230           52.78s\n",
      "        20       12902.7278           8.8442           51.01s\n",
      "        30       12678.3394           5.3381           50.55s\n",
      "        40       12584.1329           3.2789           49.58s\n",
      "        50       12494.7635           2.2431           48.73s\n",
      "        60       12362.5756           1.5135           47.92s\n",
      "        70       12299.0336           1.0706           47.17s\n",
      "        80       12355.2756           0.9342           46.75s\n",
      "        90       12249.6744           0.2201           46.15s\n",
      "       100       12277.9252           0.1382           45.51s\n",
      "       200       12058.2088          -0.4595           40.62s\n",
      "       300       12057.0749          -0.1486           34.24s\n",
      "       400       12045.6105          -0.3434           28.79s\n",
      "       500       11840.7372          -0.3389           23.71s\n",
      "       600       11701.3261          -0.4856           18.90s\n",
      "       700       11715.0476          -0.5376           14.09s\n",
      "       800       11618.4679          -0.4196            9.36s\n",
      "       900       11618.6872          -0.3790            4.66s\n",
      "      1000       11473.3906          -0.5574            0.00s\n",
      "train_loss: 0.671846, val_loss: 0.721637\ttime elapsed: 0:00:48\n",
      "accuracy score:  0.6927328889611445\n",
      "f score:  0.3401822527318355\n",
      "train_loss mean: 0.671758, val_loss mean: 0.722696\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "                            learning_rate=0.02, loss='deviance', max_depth=5,\n",
       "                            max_features=None, max_leaf_nodes=None,\n",
       "                            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                            min_samples_leaf=1, min_samples_split=2,\n",
       "                            min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "                            n_iter_no_change=None, presort='auto',\n",
       "                            random_state=36683, subsample=0.7, tol=0.0001,\n",
       "                            validation_fraction=0.1, verbose=1,\n",
       "                            warm_start=False),\n",
       " [[0.6719042190346873,\n",
       "   0.722855029270888,\n",
       "   0.3485588682695016,\n",
       "   0.6864943929790346],\n",
       "  [0.6704570671550301,\n",
       "   0.7239355797787529,\n",
       "   0.3561916920726081,\n",
       "   0.6918075422626788],\n",
       "  [0.6733620623927397,\n",
       "   0.7193758871905407,\n",
       "   0.35255309417823316,\n",
       "   0.6919200130060152],\n",
       "  [0.6712196877973301,\n",
       "   0.725677906626007,\n",
       "   0.3362617122855598,\n",
       "   0.6870427572752398],\n",
       "  [0.6718456524081521,\n",
       "   0.721636890191625,\n",
       "   0.3401822527318355,\n",
       "   0.6927328889611445]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb_clf = GradientBoostingClassifier()\n",
    "params = {'learning_rate': 0.02,\n",
    "          'n_estimators': 1000,\n",
    "          'subsample': 0.7,\n",
    "          'max_depth': 5,\n",
    "          'random_state': 36683,\n",
    "          'verbose': 1\n",
    "         }\n",
    "gb_clf.set_params(**params)\n",
    "train_cv(gb_clf, 'gb',split_data(), preprocess='no_preprocess')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
